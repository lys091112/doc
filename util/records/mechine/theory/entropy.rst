.. _records_mechine_theory_entropy:

信息熵
=======


熵是接收的每条消息中包含的信息的平均量，用来表示所有信息量的期望,又被称为信息熵、信源熵、平均自信息量,可以理解为每条信息的信息量大小


信息论之父克劳德·香农，总结出了信息熵的三条性质：

- 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。

- 非负性，即信息熵不能为负。这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。

- 累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。

所以有P(xy) = p(x) * p(y) h(x,y) = h(x) + h(y) 

一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率p(x)来度量。概率大，出现机会多，不确定性小；反之就大。从而有熵h(x)的定义应该是概率的单调函数

从而 h(x) 应该是 p(x) 的log函数，即 h(x) = log(p(x)), log的基可以是任意值

1. 熵的定义函数：

.. math::
    
    H(X)=-\sum_{i=1}^n p(x_i)log(p(x_i))

:math:`H(x)` 的取值范围为：:math:`0 <= H(x) <= log{|x|}` 其中X代表的是离散数据的取值个数

在概率归一化的限制下,使用拉格朗日乘数法可以找到熵的最大值。因此,我们要最大化

TODO: https://blog.csdn.net/pipisorry/article/details/51695283

从其图像可以看出，一个事件的发生的概率离0.5越近，其熵就越大，概率为0或1就是确定性事件，不能为我们带信息量。也可以看作是一件事我们越难猜测是否会发生，它的信息熵就越大

2. 对于买彩票这种二选一类型的，信息熵又可以写作 

.. math::

    \begin{eqnarray} H(X)&=&-\sum_{i=1}^n p(x_i)log(p(x_i))\\ &=&-p(x)log(p(x))-(1-p(x))log(1-p(x))\end{eqnarray}


相对熵（KL散度）
:::::::::::::::::

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异,用P来描述目标问题，而不是用Q来描述目标问题得到的信息增量,在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。

相对熵是非对称的，依旧是说:math:`D_{KL}(P||Q)` != :math:`D_{KL}(Q||P)`

相对熵的定义如下：

.. math::

    D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{3.1}

1. n为事件的所有可能性，即样本合集
2. :math:`D_{KL}` 的值越小，表示q分布和p分布越接近


交叉熵
:::::::

其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小,用来评价真实样本和验证值之间的相似程度，从而控制样本的训练

由相对熵可以推倒如下：

.. math::

    \begin{eqnarray}
    D_{KL}(p||q) &=& \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\\
    &=& -H(p(x))+[-\sum_{i=1}^np(x_i)log(q(x_i))]
    \end{eqnarray}

得到的结果，前一部分为信息熵H(p), 而后一部分就是交叉熵。

交叉熵的定义如下：

.. math::

    H(X)=-\sum_{i=1}^n p(x_i)log(q(x_i))

其中P是真实的分布，而Q是样本拟合分布

在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即
:math:`D_{KL}(y||\hat{y})`.由于KL散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型


信息熵和基尼系数的图像对比：
::::::::::::::::::::::::::::::::

从图中可以看出，主要区别在于，熵达到峰值的过程要相对慢一些。因此，熵对于混乱集合的判罚要更重一些,熵只依赖X的分布，和X的取值没有关系，熵是用来度量不确定性，当熵越大，概率说X=xi的不确定性越大，反之越小

|entropy|

.. |entropy| image:: ../pictures/entropy.jpeg 
