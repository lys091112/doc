.. highlight:: rst

.. _records_mechine_theory_decision_tree:

决策树
-----------

.. Tip::

    信息越换乱，那么信息所含的信息熵则越高

决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值,可以记录下被归为不可再分的集合。 `wiki <https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91>`_

决策树的目的是为来建立一个泛化能强，能够处理为遇见示例的能力。最主要的是决策树的建立，主要分一下几个方面


划分选择
=========

一般来说，随着划分的进行，剩余的数据应该尽可能的属于同一类，即节点的纯度越高越好。

信息增益
:::::::::::

在信息增益中使用的算法是信息熵，定义为 :math:`Gain(X)=-\sum_{i=1}^k p(x_i)log_2(p(x_i))` ,
Gain(X)的值越小，则Gain的纯度越高

假定离散属性a有V个可能的取值 :math:`{a_1, a_2,...,a_v}` ，那么当我们使用a属性对样本X进行划分后,则会产生V个分支（每个值一个分支）,每个分支中包含的样本数记为 :math:`X^v` ,所获得信息增益为：

.. math::

    Gain(X,a)=Gain(X) - \sum_{i=1}^v{\frac{|X_v|}{|X|}}Gain(X^v)   \tag{1.1}

其中 :math:`\frac{|x_v|}{X}` 代表的是分类后分支占样本总数的比例

一般来讲，信息增益越高，代表分类后的分支的信息熵越小（纯度越高），所以分类时，选取信息增益比较高的属性。著名的ID3决策树使用的就是信息增益为划分准则

增益率
::::::::


信息增益准则偏向与对可取值数目较多的属性有所偏好,为了减少这种偏好的影响，C4.5决策树使用来信息增益率。信息增益率的定义为：

.. math::

    Gainration(X,a) = \frac{Gain(X,a)}{IV(a)}

其中

.. math::

    IV(a) = -\sum_{i=1}^v{\frac{|X_v|}{X}log_2\frac{|X_v|}{X}}

信息增益率则对可能取值数目较少的属性有偏好，因此C4.5并不直接选择增益率最大的进行划分，而是使用了一个启发式，先从候选划分属性中挑选出 ``信息增益`` 高于平均水平的，再从中选择 ``增益率`` 最高的

基尼指数
::::::::::

基尼不纯度表示一个样本在子集中被分错的可能性，为这个样本的被选中概率和被选错概率的乘积。当节点都是一个样本时，基尼不纯度为0.  基尼不纯度的定义为：

.. math::

    Gini(X) = \sum_{k=1}^{|y|}{ \sum_{p_k^{'} \neq {p_k}}p_kp_k^{'}}

            = \sum_{k=1}^{|y|}{p_k}{(1-p_k)}

            = 1 - \sum_{k=1}^{|y|}p_k{^2}

:math:`p_k` 代表的是样本中可能取值的概率，样本的Gini越小，代表数据的纯度越高

同(1.1),经过属性a分类后的基尼指数定义为：

.. math::

    GiniIndex(X,a) = \sum_{k=1}^V{\frac{|D^v|}{|D|}Gini(D^v)}

特性：
    1、是一种不等性度量；
    2、通常用来度量收入不平衡，可以用来度量任何不均匀分布；
    3、是介于0~1之间的数，0-完全相等，1-完全不相等；
    4、总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似）

我们在候选属性A时，选择那个是的划分后基尼指数最小的属性作为划分属性。 :math:`a_*=argmin_{a \in A}GiniIndex(D,a)`

CART决策树使用了该方式来选择划分属性

剪枝处理
============

剪枝处理主要是用来解决 ``过拟合`` 问题的主要手段,因为在决策树学习过程中，为了尽可能正确的区分样本，节点划分的过程不断持续，可能会造成结果过于匹配训练样本，把训练样本的一些属性页当作所有数据都具有的一般特性而导致过拟合。 因此可通过去掉一些分支来降低 ``过拟合`` 风险

剪枝的基本策略： 预剪枝(prepruning)  后剪枝(post-pruning),
    - 预剪枝，在创建决策树的过程中判断如果当前节点划分不能带来决策树泛化性能提升，则停止划分并标记为叶子节点。
    - 后剪枝 在决策树创建好之后，自底向上对非叶子节点进行度量，如果该节点换成叶子节点能给决策树带来泛化性能提升，则提升该节点为叶子节点

一般将数据集分为两部分，一部分用于训练，一部分用于验证数据的准确性，为剪枝提供依据

预剪枝
:::::::

TODO

停止条件：

1. 节点所有观测类都属于一类
2. 节点所有观察的属性取值一致
3. 树的深度达到阙值
4. 该节点的子节点树小于观测的阙值
5. 没有属性能满足分裂准则的阙值


后剪枝
:::::::

第一种方式：
    以训练数据建立好决策树，然后通过验证数据判断当前决策树的准确精度为X.

    判断一个非叶子节点，将其置为叶子节点，然后用验证数据集判断当前的准确精度Y，
        1. 如果Y > X ,即合并后决策树的准确精度有所提高，那么该节点可以提升为叶子节点。
        2. 如果Y = X，一般情况下会进行合并，因为剪枝后的模型更好
        3. 如果Y < X, 则不尽心剪枝

    用上一个节点处理后的精度作为下一个节点判断的标准精度去比较，看下一个节点是否可以进行剪枝，以此往上处理

第二种方式：
    计算某非叶子节点合并后熵的变化有合并前的误差是否在设定值内


后剪枝通常比前剪枝保留来更多的分支，欠拟合的风险更小，泛化性能由于预剪枝。 但是后剪枝完全是在决策树建立好之后进行的，而且需要对所有的非叶子节点进行考察，因此在训练事件成本上要比位剪枝和预剪枝的决策树要大很多。 


连续和缺失值
====================

连续型数据
::::::::::::

1. 对数据排序，然后相邻的俩俩取平均值，然后以每一平均值作为划分点，查看划分后的熵，找出最合适的值(即增益最高的) C4.5算法使用
    - 对n个值求中间值，得（n-1）个划分点 :math:`T_a = \bigg\{ \frac{a^i + a^{i+1}}{2} | 1 \leqslant i \leqslant {n-1} \bigg\}` .

    - 像处理离散点一样，考察各个点的划分。取 ``t`` 将数据分为 :math:`{X_t}^-` 和 :math:`{X_t}^-`,分别包含的是划分点中大于t和小于t的两部分集合。 信息增益为：

.. math::

    Gain(X,a)=\max_{t \in T_a} Gain(X,a,t)
        = \max_{t \in T_a} Ent(X) - \sum_{ \lambda \in {-,+}}{\frac{|{X_t}^ \lambda|}{|X|}}Ent({X_t}^\lambda)   
    
其中 :math:`Gain(X,a,t)` 代表的是样本X基于t划分后的信息增益 \
     :math:`Ent(X)` 是样本X含有的信息熵 \

另外，连续型的数据在其子类节点中是可以不断的再次分割的

2. 均方差计算,计算出数据的均方差，然后找出紧邻均方差值的两个值作为左右俩分支
   .. https://www.jianshu.com/p/7fbff1714287


缺失值
::::::::

在处理样本数据时，样本的属性往往不是完整的，可能会有缺失，因此需要对这些缺失值做处理

两个问题：1. 如何在属性值缺失的情况下进行划分属性选择 2. 给定划分属性，如何在属性缺失的情况下对样本进行划分

问题1:
给定训练样本X 和属性a，令 :math:`\tilde{X}` 代表在属性a上没有缺失值的样本集，假设a属性有V个可能的取值，那么有三个需要关注的值。

 1. 除缺失值以外，有值的数据占总数据的比例 :math:`p=count(\tilde{X})/ count(X)` \
 2. 没有缺失值的数据中，最终的目标集(最终的分类值)的占比 :math:`\tilde{p}_k` \
 3. 没有缺失值的样本中，a属性可取值 :math:`a^v` 样本的占比 :math:`\tilde\gamma_v` \

那么我们计算信息增益的公式为

.. math:: 

    Gain(X,a) = p * Gain(\tilde{X},a)

        = p * \bigg( Ent(\tilde{X}) - \sum_{v=1}^V \tilde{\gamma}_v Ent(\tilde{X}^v) \bigg)


C4.5使用上述该算法进行决策树的划分, 具体参考 周志华-机器学习P97

问题2: 针对于有缺失值的数据,确定其划分

当面临缺失值问题时，可以选择让节点走所有可能的分支，然后分别得出各自的结果集，然后去平均计算那个结果集占总集的比例最高，那么那个可能性就越高

随机森林
==========
