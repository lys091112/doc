##  回归使用的优化算法

1. [最小二乘法](#最小二乘法)
2. [梯度下降法](#梯度下降法)


常见的多元线性方程如下：

   $$ \hat{y_i} (\theta,x_i) = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2} + \dots + \theta_n x_{in} $$
其中:
 $\hat{y}$ 是预测值

 $\theta_1, \dots, \theta_n$ 是系数

 $x_{i0}, x_{i1}, x_{in}$ 是自变量

 回归的目的是为了让预测值与实际值之间的误差尽可能的小，评估方式是所有误差的平方和,常使用的方法有 最小二乘法 和 梯度下降法

### 最小二乘法

最小二乘法的思想: 求多元函数极值, 公式如下：

$$ e_i = y_i - \hat{y_i} $$
   $$ \hat{y_i} (\theta,x_i) = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2} + \dots + \theta_n x_{in} $$

   $$e_i =  y_i - (\theta_0 + \theta_1 x_{i1} + \dots + \theta_n x_{in})  $$
   $$ W = \sum\limits_{i=1}^{n} e_i^2 = \sum\limits_{i=1}^{n} (y_i - (\theta_0 + \theta_1 x_{i1} + \dots + \theta_n x_{in}))^2 $$

最后使所有的偏导等于0,求出系数

$$ \frac {\partial e} {\partial \theta_0} = 0  $$    
$$ \frac {\partial e} {\partial \theta_1} = 0  $$    
$$ \frac {\partial e} {\partial \theta_2} = 0  $$    
$$ \dots $$
$$ \frac {\partial e} {\partial \theta_n} = 0  $$    

或者使用向量的表示方法：

$$ Q = \sum\limits_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum\limits_{i=1}^{n} (y_i - (b - a x_i))^2 $$

$$ a = \frac {\sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})} {\sum\limits_{i=1}^{n} (x_i - \bar{x})^2}
     = \frac {\sum\limits_{i=1}^{n} x_i y_i - n \bar{x} \bar{y}} { \sum\limits_{i=1}^{n} x_i^2 - n\bar{x}^2 }
$$

$$ b= \bar{y} - a \bar{x} $$

其中： $ \bar{x} = \frac {x_1 + x_2 + \dots + x_n} {n}$



### 梯度下降法

梯度下降法的评估函数为: $ \jmath (\theta) =\mathop{\min}\limits_{\theta} \frac {1}{2n} \sum\limits_{i=1}^{n} (h_\theta (x^{(i)}) - y^{(i)})^2 $

梯度更新函数为： $ \theta_{j} = \theta_{j} - \alpha \frac {\partial \jmath (\theta)} {\partial \theta_j} $

推导如下：

$$ \frac {\partial}{\partial \theta_j} \jmath (\theta) $$
$$ = 2 * \frac {1}{2n} \sum\limits_{i=1}^{n} (h_\theta (x^{(i)}) - y^{(i)}) \frac {\partial} {\partial \theta_j} (h_\theta (x^{(i)}) - y^{(i)}) $$ 
$$
= \frac{1}{n} \sum\limits_{i=1}^{n} (h_\theta (x^{(i)}) - y^{(i)}) \frac {\partial} {\partial \theta_j} (\theta_0 + \theta_1 x_{i1} + \dots + \theta_n x_{in})
$$

$$ = \frac{1}{n} \sum\limits_{i=1}^{n} (h_\theta (x^{(i)}) - y^{(i)}) x^{(i)}_{j}$$

另一种方式：

  $$\frac {\partial \jmath(\omega)} {\partial \omega_j} $$ 
  $$ = \frac{\partial}{\partial \omega_j} \frac {1}{2m} \sum\limits_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2$$
  $$ = 2 \frac{1}{2m} \sum\limits_{i=1}^{m} (f(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial \omega_j} (\sum\limits_{j=0}^{n} \omega_j x_j^{(i)} - y^{(i)}) $$
  $$ = \frac{1}{m} \sum\limits_{i=1}^{m} (f(x^{(i)}) - y^{(i)}) x^{(i)}_j $$
  $$ = \frac{1}{m} \sum\limits_{i=1}^{m}(\sum\limits_{j=0}^{n} \omega_j x_j^{(i)} - y^{(i)})x^{(i)}_j $$

  从而得：

  $$ \omega_j = \omega_j - \alpha * \frac {1}{m}\sum\limits_{i=1}^{n}(\sum\limits_{j=0}^{n} \omega_j x_j^{(i)} - y^{(i)})x^{(i)}_j $$

#### 梯度下降公式

// TODO
 依据泰勒公式，梯度的原则是每次更新都会减少，
 由一元泰勒公式得： $f(x) = f(x_0) + (x-x_0)f^{'}(x_0) $ 将末尾项尽可能的较少，即和梯度的反方向下架最快，也能获得最多收益，
 从而得出梯度的下降公式，（TODO 待证明)
 由二元级泰勒公式得：

#### 常见的几种梯度下降法

1. 批量梯度下降算法（Batch Gradient Descent)

    批量梯度下降算法，是在整个训练集上计算的，如果数据集比较大，可能会面临内存不足问题，而且其收敛速度一般比较慢

2. 随机梯度下降算法（Stochastic GradientDescent）

    是针对训练集中的一个训练样本计算的，又称为在线学习，即得到了一个样本，就可以执行一次参数更新。所以其收敛速度会快一些，但是有可能出现目标函数值震荡现象，因为高频率的参数更新导致了高方差

3. 梯度下降算法（Mini-batch Gradient Descent)

    小批量梯度下降算法是折中方案，选取训练集中一个小批量样本计算，这样可以保证训练过程更稳定，而且采用批量训练方法也可以利用矩阵计算的优势

4. 冲量梯度下降算法(Momentum optimization)

    基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下：

5. NAG

    NAG算法全称Nesterov Accelerated Gradient,对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项

6. AdaGrad

    Duchi在2011年提出的一种学习速率自适应的梯度下降算法。在训练迭代过程，其学习速率是逐渐衰减的，经常更新的参数其学习速率衰减更快，这是一种自适应算法

    AdaGrad其学习速率实际上是不断衰减的，这会导致一个很大的问题，就是训练后期学习速率很小，导致训练过早停止，因此在实际中AdaGrad一般不会被采用

7. RMSprop

    RMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减

8. Adam

    Adam全称Adaptive moment estimation，是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体

#### 学习速率

对于梯度下降算法，这应该是一个最重要的超参数。如果学习速率设置得非常大，那么训练可能不会收敛，就直接发散了；如果设置的比较小，虽然可以收敛，但是训练时间可能无法接受；如果设置的稍微高一些，训练速度会很快，但是当接近最优点会发生震荡，甚至无法稳定。不同学习速率的选择影响可能非常大
在使用中，优先选择学习速率自适应的算法如RMSprop和Adam算法，大部分情况下其效果是较好的。特别注意学习速率的问题。
还有很多方面会影响梯度下降算法，如梯度的消失与爆炸，这也是要额外注意的。

``梯度下降算法目前无法保证全局收敛还将是一个持续性的数学难题。``

### 比较

##### 最小二乘法：

    一次计算即可得到最优解(全局最优解)，但极小值为全局最小值；
    当特征数量  大于10000时，因计算矩阵逆的时间复杂度(  )会很大；
    只适用于线性模型，不适用于逻辑回归等其他模型。

#### 梯度下降法：

    需要选择学习率 ，需要多次迭代找到最优解(局部最优解)，极小值为局部最小值；
    当特征数量  大于10000时，也可以进行计算；
    适用于各种类型的模型

参考链接：
 [一文看懂常用的梯度下降算法](https://blog.csdn.net/u013709270/article/details/78667531)
