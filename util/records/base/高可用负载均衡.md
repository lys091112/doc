### Nginx LVS KeepAlive

Ngnix 不支持分布式，因此会存在单点问题，常用的集群方式是LVS + Nginx + KeepAlive.

### Lvs

 ``lvs`` 是一款用于四层负载均衡的工具。所谓的四层负载均衡，对应的是网络七层协议，常见的如HTTP协议是建立在七层协议上的，而lvs作用于四层协议上，也即：传输层，网络层，数据链路层和物理层。这里的传输层主要协议有TCP和UDP协议，也就是说lvs主要支持的方式是TCP和UDP。也正是因为lvs是处于四层负载均衡上的，因而其处理请求的能力比常见的服务器要高非常多，比如nginx的请求处理就是建立在网络七层上的，lvs的负载均衡能力是nginx的十倍以上

 ``TIP``:

    网络7层模型：
        物理层 --> 建立、维护、断开物理连接
        数据链路层 --> 建立逻辑连接、进行硬件地址寻址、差错校验,将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正
        网络层 -->进行逻辑地址寻址，实现不同网络之间的路径选择(协议：ICMP IGMP IP(IPV4 IPV6))
        传输层 -->定义传输数据的协议端口号，以及流控和差错校验(TCP UDP，数据包一旦离开网卡即进入网络传输层)
        会话层 --> 建立、管理、终止会话(对应主机进程，指本地主机与远程主机正在进行的会话)
        表示层 --> 数据的表示、安全、压缩(JPEG、ASCll、DECOIC、加密格式等)
        应用层-->网络服务与最终用户的一个接口(协议：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP)

        后三层可以合并为应用层

#### Lvs 使用的三种负载均衡方式

1. 网络地址转换（NAT）

    NAT模式下的服务器节点使用的是私有IP，均衡器是集群的唯一出口，网络结构呈现一种类似防火墙的私有网络结构，服务器节点无法和客户端直接通信，所有数据都需要经过均衡器进行处理

2. IP隧道(IP TUN )

    在LVS(NAT)模式的集群环境中，由于所有的数据请求响应的数据包都需要经过LVS调度器转发，如果后端服务器数量大于10台，则调度器就会成为整个集群环境中的瓶颈。
   数据请求包远远小于响应数据包的大小。因为响应数据包中包含有客户需要的具体数据，所以LVS(TUN)的思路就是将响应数据分离，让调度器仅处理数据请求，而让真实服务器将响应包直接返回给客户端。服务器节点拥有合法的公网IP，负载均衡器和服务器节点的连接可以是同一LAN上，也可以跨越WAN在不同的网段上

3. 直接路由（DR）

    在LVS（TUN）模式下，由于需要在LVS调度器与真是服务器之间创建隧道连接，这样同样会增加服务器的负担。与LVS(TUN)类似，DR模式也叫直接路由模式。该模式中LVS依然仅承担数据的入站请求以及根据算法选出合理的真实服务器，最终由后端真实服务器负责将响应数据包发送返回给客户端。

    负载均衡器接收到客户端请求数据包后，选择合适的服务器节点，将请求包的MAC地址改写为目的服务器节点的MAC地址，再将此包广播到服务器节点所在网段。每个服务器节点都设定一个虚拟的网络设备（lo:0），这个设备绑定了和均衡器一样的IP，只是改设备并不响应VIP的ARP解析，不会和均衡器的VIP产生地址冲突。节点服务器收到符合自身MAC的IP包后，经过处理后直接将应答数据返回客户

4. fullNAT

    无论是DR还是NAT模式，不可避免的都有一个问题：LVS和RS必须在同一个VLAN下，否则LVS无法作为RS的网关

    在包从LVS转到RS的过程中，源地址从客户端IP被替换成了LVS的内网IP。 内网IP之间可以通过多个交换机跨VLAN通信, 当RS处理完接受到的包，返回时，会将这个包返回给LVS的内网IP，这一步也不受限于VLAN。LVS收到包后，在NAT模式修改源地址的基础上，再把RS发来的包中的目标地址从LVS内网IP改为客户端的IP。

    Full-NAT主要的思想是把网关和其下机器的通信，改为了普通的网络通信，从而解决了跨VLAN的问题。采用这种方式，LVS和RS的部署在VLAN上将不再有任何限制，大大提高了运维部署的便利性

##### 四种模式的性能比较：
因为DR模式，IP TUNELL模式都是在package in 时经过LVS ；在package out是直接返回给client；所以二者的性能比NAT 模式高；但IP TUNNEL 因为是TUNNEL 模式比较复杂，其性能不如DR模式；FULL NAT模式因为不仅要更换DST IP还更换SOURCE IP所以性能比NAT下降10%。
所以，4中模式的性能如下：DR  –> IP TUNNEL  —>NAT —–>FULL NAT

[参考LVS负载均衡系统](https://www.cnblogs.com/sellsa/p/5402459.html)

#### Session 一致性

    客户端与服务端的通信，一次请求可能包含多个TCP包，LVS必须保证同一连接的TCP包，必须被转发到同一台RS，否则就乱套了。为了确保这一点，LVS内部维护着一个Session的Hash表，通过客户端的某些信息可以找到应该转发到哪一台RS上

#### LVS负载均衡调度算法

1、轮询调度（RR）

轮询算法假设所有的服务器处理请求能力都是一样，调度器会将所有请求平均分配给每个真实服务器

2、加权轮询调度(WRR)

考虑每天服务器的性能，并为每台服务器添加一个权值，权值越高的服务器，分配处理的请求越多

3、最小连接调度（LC）

把请求调度到连接数量最小的服务器上

4、加权最小连接调度（WLC）

给每个服务器一个权值，调度器会尽可能保持服务器连接数量与权值之间的平衡

5、基于局部性的最少连接调度（lblc）

根据请求的目标IP地址寻找最近目标IP地址所使用的服务器，如果这台服务器依然可用，并且有努力处理该请求，调度器会尽量选择相同的服务器，否则会继续选择其他可行的服务器

6、带复制性的基于局部的最少连接调度（lblcr）

它记录的不是一个目标IP与一台服务器之间连接记录，它会维护一个目标IP到一组服务器之间的映射关系，防止单点服务器负载过高

7、目标地址散列调度（DH）

根据目标IP地址通过散列函数将目标IP与服务器建立映射关系，出现服务器不可用或负载过高的情况下，发往该目标的IP请求会固定发给改服务器

8、源地址散列调度（SH）

根据源地址散列散发进行静态分配固定的服务器资源

#### LVS单点问题

1。 keepalive + lvs  

    能够提高集群的高可用性并增加后端检测功能、简化配置，满足常规需求,但Keepalived-LVS集群中，同一个VIP只能由一台设备进行宣告，为一主多备的架构，不能横向拓展集群的性能

2. lvs + ospf

    OSPF 即(ECMP（Equal-CostMultipathRouting）等价多路径)，存在多条不同链路到达同一目的地址的网络环境中，如果使用传统的路由技术，发往该目的地址的数据包只能利用其中的一条链路，其它链路处于备份状态或无效状态，并且在动态路由环境下相互的切换需要一定时间，而等值多路径路由协议可以在该网络环境下同时使用多条链路，不仅增加了传输带宽，并且可以无时延无丢包地备份失效链路的数据传输。
    特点：
    - 基于流的均衡负载
    - 最大链路数受设备限制（最高16）
    - 所有链路都active，故障链路自动剔除

[参考LVS-ospf集群](http://noops.me/?p=974)

## Nginx + keepalive

####  Nginx

  nginx是一款非常优秀的反向代理工具，支持请求分发，负载均衡，以及缓存等等非常实用的功能。在请求处理上，nginx采用的是epoll模型，这是一种基于事件监听的模型，因而其具备非常高效的请求处理效率，单机并发能力能够达到上百万。nginx接收到的请求可以通过负载均衡策略分发到其下一级的应用服务器，这些服务器一般是以集群方式部署的，因而在性能不足的情况下，应用服务器可以通过加机器的方式扩展流量。此时，对于一些特大型的网站，性能的瓶颈就来自于nginx了，因为单机的nginx的并发能力是有上限的，而nginx本身是不支持集群模式的，因而此时对nginx的横向扩展就显得尤为重要

### keepalived

  keepalived是一款服务器状态检测和故障切换的工具。在其配置文件中，可以配置主备服务器和该服务器的状态检测请求。也就是说keepalived可以根据配置的请求，在提供服务期间不断向指定服务器发送请求，如果该请求返回的状态码是200，则表示该服务器状态是正常的，如果不正常，那么keepalived就会将该服务器给下线掉，然后将备用服务器设置为上线状态

  Lvs + keepalived 可以实现 lvs 的故障检测和服务切换
  nginx + keepalived 可以实现nginx的故障检测和服务切换

### 常见的负载均衡

#### 按层分类：

1. 二层负载均衡（mac）
    根据OSI模型分的二层负载，一般是用虚拟mac地址方式，外部对虚拟MAC地址请求，负载均衡接收后分配后端实际的MAC地址响应）

2. 三层负载均衡（ip）
    一般采用虚拟IP地址方式，外部对虚拟的ip地址请求，负载均衡接收后分配后端实际的IP地址响应）

3. 四层负载均衡（tcp）
    在三次负载均衡的基础上，用ip+port接收请求，再转发到对应的机器

4. 七层负载均衡（http）
    根据虚拟的url或IP，主机名接收请求，再转向相应的处理服务器

##### 4层均衡
    四层的负载均衡就是基于IP+端口的负载均衡：在三层负载均衡的基础上，通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡
    对应的负载均衡器称为四层交换机（L4 switch），主要分析IP层及TCP/UDP层，实现四层负载均衡。此种负载均衡器不理解应用协议（如HTTP/FTP/MySQL等等

    F5：硬件负载均衡器，功能很好，但是成本很高。
    lvs：重量级的四层负载软件
    nginx：轻量级的四层负载软件，带缓存功能，正则表达式较灵活
    haproxy：模拟四层转发，较灵活

##### 7层均衡

  基于虚拟的URL或主机IP的负载均衡
  对应的负载均衡器称为七层交换机（L7 switch），除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息，实现七层负载均衡

    haproxy：天生负载均衡技能，全面支持七层代理，会话保持，标记，路径转移；
    nginx：只在http协议和mail协议上功能比较好，性能与haproxy差不多；
    apache：功能较差
    Mysql proxy：功能尚可

一般是lvs做4层负载；nginx做7层负载；haproxy比较灵活，4层和7层负载均衡都可

#### 按物理单元分类

1. DNS 负载均衡

优点：
    - 将负载均衡的工作交给DNS，省去了网站管理维护负载均衡服务器的麻烦。
    - 技术实现比较灵活、方便，简单易行，成本低，使用于大多数TCP/IP应用。
    - 对于部署在服务器上的应用来说不需要进行任何的代码修改即可实现不同机器上的应用访问。
    - 服务器可以位于互联网的任意位置。
    - 同时许多DNS还支持基于地理位置的域名解析，即会将域名解析成距离用户地理最近的一个服务器地址，这样就可以加速用户访问，改善性

缺点：
    - 目更新不及时: 前的DNS是多级解析的，每一级DNS都可能缓存A记录，当某台服务器下线之后，即使修改了A记录，要使其生效也需要较长的时间，这段时间，DNS仍然会将域名解析到已下线的服务器上，最终导致用户访问失败。
    - 分配策略比较简单：不能够按服务器的处理能力来分配负载。DNS负载均衡采用的是简单的轮询算法，不能区分服务器之间的差异，不能反映服务器当前运行状态，所以负载均衡效果并不是太好。
    - 可能会造成额外的网络问题。为了使本DNS服务器和其他DNS服务器及时交互，保证DNS数据及时更新，使地址能随机分配，一般都要将DNS的刷新时间设置的较小，但太小将会使DNS流量大增造成额外的网络问题
    - 扩展性差：DNS 负载均衡的控制权在域名商那里，无法根据业务特点针对其做更多的定制化功能和扩展特性

2. 硬件负载均衡

    硬件负载均衡是通过单独的硬件设备来实现负载均衡功能，这类设备和路由器、交换机类似，业界典型的硬件负载均衡设备有两款：F5 和 A10
    优点：
    - 功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡。
    - 性能强大：对比一下，软件负载均衡支持到 10 万级并发已经很厉害了，硬件负载均衡可以支持 100 万以上的并发。
    - 稳定性高：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。
    - 支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防 DDoS 攻击等安全功能

缺点:
    - 价格昂贵
    - 扩展能力差：硬件设备，可以根据业务进行配置，但无法进行扩展和定制

在大型公司通常可以考虑硬件负载+软件负载

即便是硬件负载，如果只使用一台也会出现单点问题，所以一般都会同时使用多台。

// TODO F5负载的常用转发模式

3. 软件负载 lvs nginx 


#### 应用场景

1. 应用于高访问量的业务
2. 横向扩张系统
3. 消除单点故障
4. 同城容灾
